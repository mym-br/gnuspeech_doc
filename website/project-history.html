
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>gnuspeech project history</title>
</head>

<body>

<H2>Brief <I>gnuspeech</I> History</H2>

<P>The current state and a little history of the project are as follows. The descriptions also provides a reference for the original system components that are available in the <I>NeXT</I> source archive in the project SVN Repository, and to understand the scope of <I>gnuspeech</I>.
</P>
<UL>
<LI><B>Monet:</B><BR>
The interactive language database creation and testing tool used to create the original databases for English text-to-speech conversion using the new articulatory model of the vocal tract (the <I>TRM</I>).  The ported version of <I>Monet</I> translates symbolic <I>or</I> punctuated text input strings into the digital waveform needed to &#8220;speak&#8221; the input text (since it now also uses <I>Parser</I>, shown only as part of the <I>TextToSpeech Server</I> in the overview figure <A HREF="http://www.gnu.org/software/gnuspeech/"> on the project Home Page</A>). It also provides interactive facilities to create and modify the database (rules, posture-specifications, and the like) that are used by the <I>TextToSpeech Server</I>. Finally it provides facilities for experimenting with and controlling the parameters that manage the intonation as well as allowing the contour to be edited. Although the code and interfaces exist for the database editing and intonation experiments, that prime function of <I>Monet</I> is not yet fully operational and is the most important task to finish to support <I>Monet&#8217;s</I> real purpose. The work mostly involves adding the file writing. The speech output and parameter displays, as well as the text parsing and application of the rhythm and intonation rules are complete. <I>Monet</I> was originally designed and developed by Craig Schock to create the databases need to support David Hill&#8217;s &#8220;event-based&#8221; approach to speech synthesis, with testing and suggestions for improvements by David Hill and Leonard Manzara. Is was the essential in-house tool for the original <I>Trillium</I> development effort. A valuable enhancement of <I>Monet</I> would be a meta-layer to allow <I>TRM</I> posture data to be specified in terms of such linguistically oriented parameters as lip opening, jaw rotation and tongue height, and which would then translate these high-level parameters into the current &#8220;low-level&#8221; <I>TRM</I> section radius data.

<LI><B>The Tube Resonance Model (TRM) (and associated TRM Control Model):</B><BR>
This is a &#8216;C&#8217; implementation of the tube model that forms the core of the synthesis system, and was created by Leonard Manzara, who also ported it to the DSP56001 signal processor and made it run in real-time on the <I>NeXT</I>.  It is based on work by Perry Cook and Julius Smith at the <I>Stanford University Center for Computer Research in Music and Acoustics</I> (CCRMA). The <I>TRM</I> and associated <I>TRM Control Model</I> are both fully ported and working&#8212;supporting <I>Monet</I> and the <I>TextToSpeech Server</I>. The <I>TRM</I> without the <I>TRM Control Model</I> is used by <I>TRAcT</I>. The <I>TRM</I> comprises just ten oral tube sections, with a splitting junction at the velum, and five nasal sections. The <I>TRM Control Model</I> configures the ten oral sections into the eight regions of different length required by the <I>DRM</I>. This is an approximation, but close and very effective. Thirty regions (requiring considerably more computation) could be grouped into an exact rendition of the <I>DRM</I>. A <A HREF="./trm-write-up.pdf">description of the TRM is available here</A> (Use the back button to return).

<LI><B>TRAcT:</B> (originally named Synthesizer<BR>
This is not a text-to-speech synthesiser!  It is a GUI application that allows a user, usually a language developer or someone interested in the behaviour of the <I>TRM</I>, to interact directly with the <I>TRM</I>, listen to the output under different <I>static</I> conditions, and analyse the output.  It was an important tool used in developing the databases for <I>Trillium&#8217;s</I> original English text-to-speech system because it allowed the tube configurations needed to define the speech &#8220;postures&#8221; (of the vocal tract) to be explored and finalised.  Although it does have built-in analysis and display features, it was also used in conjunction with a Kay <I>Sona-Graf</I> spectrum analyser in order to compare the spectral analyses of sounds produced by proposed &#8220;postures&#8221; with what was seen in natural speech.  The <I>Sona-Graf</I> was also used to check the synthetic speech output from <I>Monet</I> against the same utterances in natural speech.  <I>TRAcT</I> has been mostly ported to the Mac under OS/X by David Hill. Not all the display functions are working and some clean up is needed, but it is basically functional.  The original version of <I>TRAcT</I> (<I>Synthesizer</I>) was created (for the <I>NeXT</I>) by Leonard Manzara.

<LI><B>PrEditor:</B><BR>
<I>PrEditor</I> is an application to allow users to create and maintain their own dictionaries.  The original <I>TextToSpeech Kit</I> looks up several dictionaries in the order User &#8594; Application &#8594; Main.  <I>PrEditor</I> allows the User and Application dictionaries to be created and maintained by the user or application developer respectively.  An initial port was begun by Eric Zoerner but is not yet functional.  The original <I>PrEditor</I> on the <I>NeXT</I> was written by Vince DeMarco and David Marwood, documented by Leonard Manzara and later upgraded by Michael Forbes.

<LI><B>The Main Dictionary:</B><BR>
This has not really changed since the original NeXT implementation and is used by the <I>Parser</I> (and hence is also used indirectly by <I>Monet</I>).  It is a compromise pronunciation between British (RP) English&#8212;mainly the vowels and related stuff, and General American&#8212;especially the rhotic &#8220;r&#8221; sound.  It includes more than 70,000 words, plus facilities for creating/checking derivatives such as plurals and adverbs, plus information concerning word stress and part-of-speech.  The part-of-speech information is not used, but is included to allow future development of grammatical processing.  The main dictionary was compiled mainly by David Hill, after a preliminary version plus creation tools were set up by Craig Schock. It would be well worth creating two versions, one for British English and the other for American English, though the current combined version is surprisingly acceptable, especially given the excellent rhythm and intonation.

<LI><B>BigMouth:</B><BR>
This is an application that allows text-to-speech to be tried out without reference to any particular application on the <I>NeXT</I> and also drives the &#8220;Speech Service&#8221;. There is speech service on the OS X version, that has to be installed, and is activated through the OS X <I>&#8220;Services Preferences &#8230; &#8221;</I> menu item under the [Application Name] &#8594; Services menu in the top menu bar.  It uses the <I>TextToSpeech Server</I>.  The original source for <I>BigMouth</I> was created by Leonard Manzara. There was another applet with a similar name on the <I>NeXT</I> that has nothing whatever to do with <I>gnuspeech</I>.

<LI><B>The TextToSpeech Server (TTS Server):</B><BR>
The original <I>NeXT</I>-based <I>TextToSpeech Kit</I> came in three versions.  The <I>User Kit</I> which simply provided speech output as a service available to any application; the <I>Developer Kit</I> which provided the means to incorporate speech into applications directly; and the <I>Experimenter Kit</I> which allowed full access to all the tools used by Trillium in developing language databases including dictionaries.  All of these used the <I>TextToSpeech Server</I> for the actual conversion of text to speech output.  This real-time task was made possible on the <I>NeXT</I>, which is relatively slow by today&#8217;s standards, by using the built-in DSP (a Motorola DSP-56001).  In the Mac OS X and GNU/Linux GNUStep versions of <I>Monet</I> and <I>Synthesizer</I>, the host computer performs all the computation&#8212;as CPU speeds are at least two orders of magnitude faster than the old <I>NeXT</I>.  The use of the DSP on the NeXT gave a clear separation between the text-to-speech tasks associated with creating the event framework for synthesis, and the those associated with transforming the event framework into the digital speech waveform, and outputting it&#8212;the latter tasks being carried out by the DSP-based version of the <I>TRM</I> and <I>TRM Control Model</I>.  Thus the <I>TRM</I> ran on the DSP in real-time and communicated by DMA access.  There was also a &#8216;C&#8217; version of the tube model which could not run in real-time&#8212;what we called the <I>Software Synthesiser</I>.  It was useful for producing a slightly higher quality of speech since it did not have to be squeezed into the DSP and rigorously optimised because of the marginal ability (even on the DSP) to run in real-time.  This &#8216;C&#8217; version of the <I>TRM</I> is what forms the basis of the current ports&#8212;possible now because of the greatly increased processor speeds over the last 20 years. The <I>TextToSpeech Server</I> is complete.
</P>

<P>
No database creation and manipulation components or interactive interfaces are provided for the <I>TextToSpeech Server</I> itself. Those are only appropriate for <I>Monet</I> and other applications that use it.  However, provision <I>is</I> made to set the parameters for controlling static aspects of the synthesis (tube length, mean pitch, and so on&#8212;the so-called &#8220;utterance-rate parameters&#8221;). These static parameters are normally held in a system library as a &#8220;defaults database&#8221;. This refinement is not yet included in the ports but is a function of <I>ServerTest</I> (see below). The <I>Text-to-Speech Server</I> computes the event framework from the input text via the intermediate input syntax produced by the <I>Parser</I>.  This pre-processing includes dictionary look-up to get the correct pronunciation. There is no significant parsing in terms of normal English grammar, and no attempt is made to determine meaning (which would allow different pronunciations of words with the same spelling to be disambiguated, and would to allow slightly more accurate rhythm and intonation to be generated). Such abilities should eventually be added. The word stress information from the dictionary is used to help determine the rhythmic framework according to the Jones/Abercrombie/Halliday (British) &#8220;tendency-towards-isochrony&#8221; theory of British English speech by placing &#8220;foot&#8221; boundaries before the word stress in words having word-stressed syllables. &#8220;<Isochrony</I>&#8221; implies equal length rhythmic units but, in practice, there is considerable difference in their length, depending on the stress and the number of postures involved. However, longer stressed rhythmic units are found to be somewhat shorter than would be expected from their component postures. This is what is meant by the &#8220;isochrony effect&#8221;. The normal punctuation is also used in this process, which allows a distinction to be made between statements, emphatic statements, questions, and questions expecting a yes/no answer, for the purpose of selecting different intonation contours. The rhythm and intonation models are very effective, producing speech that is pleasant to listen to.  However, without using knowledge of meaning, it is hard to decide on the placement of the tonic (information point) of the phrase or sentence, or to disambiguate words which are spelled the same but pronounced differently. As a default, the tonic is placed in phrase/sentence-final position.  This causes some undesirable compromise of the speech rhythm and intonation when it is wrong. Some measure of grammatical analysis and understanding would be a most effective refinement of the current system. Work on the rhythm and intonation was carried out by a number of contributors, including David Hill, Ian Witten, Neal Reid, and Wiktor Jassem, based on the published work of others, as well as on in-house experiments.

<BR>
&#8195; There's a <A HREF="http://www.gnu.org/software/gnuspeech/">diagram of the relationships between the various TTS components of the complete system</A>  on the project Home Page.


<LI><B>ServerTest and ServerTestPlus:</B><BR>
These applications were designed to allow the <I>TextToSpeech Server</I> to be tested and, in the case of the <I>TextToSpeech Server Plus</I>, provide certain &#8220;hidden&#8221; methods that were restricted to Trillium's &#8220;in-house&#8221; use. Now that the whole system is available under a GPL, the restricted &#8220;ServerTest&#8221; version is obsolete and the name <I>ServerTest</I> will refer to a reimplementation of <I>ServerTestPlus</I>.  For example, one of the 18 originally-hidden methods allowed plain text to be converted into the intermediate <I>Monet</I> input syntax. The ported version of <I>gnuspeech</I> incoporates use of the <I>Parser</I> in various accessible roles. It was originally hidden to keep the main dictionary material proprietary, as it could have been used to completely decode the encoded dictionary.

<LI><B>WhosOnFirst:</B><BR>
<I>WhosOnFirst</I> was the first publicly available software associated with the Trillium <I>TextToSpeech</I> system and was designed as a bit of a teaser.  As issued, it provided indication, on the <I>NeXT</I> console, of remote logins.  It also told the user that, if they had the Trillium <I>TextToSpeech</I> system, they could get voice alerts not only to remote logins, but other system activity such as application launches.  <I>WhosOnFirst</I> was written by Craig Schock and was instrumental in catching and identifying a hacker trying to break into our system soon after it was set up.  <I>WhosOnFirst</I> has not yet been ported.

<LI><B>say:</B><BR>
A command line interface to the <I>TextToSpeech Server</I> that can be used from a terminal or in shell scripts.  It was written by Craig Schock and has not been ported yet, though there is a similar facility for the GNU/Linux GNUStep version.

<LI><B>SpeechManager:</B><BR>
The <I>SpeechManager</I> was provided to allow the <I>TextToSpeech Server</I> operating system parameters to be optimised for different systems, because no particular setting of priorities, initial silence fill, and so on, could be right for all systems.  In particular, in networked systems, or systems with a high compute load from other tasks, the speech would sometimes crackle due to interference from other tasks.  The <I>SpeechManager</I>, which could only be run as root, allowed the <I>TextToSpeech Server</I> to be restarted, and the various parameters controlling priority and so on to be set to new values to avoid crackling whilst minimising the use of system resources. These functions are almost certainly obsolete these days, given the increased compute-power available.  Some functions (such as reporting the version of the main dictionary in use, or restarting the <I>TextToSpeech Server</I>) may still be required in some form. The <I>SpeechManager</I> was written by Craig Schock.  It has not been ported.

<LI><B>SpeechRegistrar:</B><BR>
An applet that was provided to allow any of the <I>TextToSpeech Kits</I> to be registered, using a password, and was run under the root account.  The original function is now obsolete, but may be useful, in revised form, as a way of building user groups for the ported system.  It was written by Craig Schock.  It has not been ported.

<LI><B>TrilliumSoundEditor:</B><BR>
The <I>TrilliumSoundEditor</I> is speech editor and analysis program intended to provide a more versatile replacement for the publicly available <I>Sonagram</I> program written by Hiroshi Momose.  Although <I>TrilliumSoundEditor</I> was never completely finished, it provided the basic spectrographic analysis functionality required for speech development and could be finished/upgraded/ported at some point in the future.  The program was written by Craig Schock.  None of the <I>TrilliumSoundEditor</I> has yet been ported, but the source is available. With the advent of <I>Praat</I> (see the <I>Monet</I> manual in the initial distribution of gnuspeech), the <I>Trillium Sound Editor</I> is probably redundant.
</UL>

<P>
In summary, much of the core software has been, and some is being ported to the Mac under OS/X, and GNU/Linux under GNUStep. All sources and builds for the current work are currently in the Git repository, with older material in the SVN repository under three branches (for the <I>Next</I>, Mac OS X, and GNU/Linux under GNUStep versions&#8212;see below). Speech may be produced from input text. The development facilities for managing and creating new language databases, or modifying the existing English database for text-to-speech lack mainly the file writing components. The <I>gnuspeech</I> facilities also provide the tools needed for psychophysical and linguistic experiments. <I>TRAcT</I>, which gives direct access to the tube model, functional&#8212;a few of the logarithmic data displays remain to be finished, and clean-up is needed. Some accessory tools are available. As well as the acknowledgements above, Greg Casamento, Adam Fedor and the Savannah Hackers provided valuable support getting the <I>gnuspeech</I> project established, as well as initial work that facilitated the port, including making ubiquitous and tedious changes to the entire <I>NeXT</I> source code to bring it up to <I>OpenStep</I> standards. This work and support is gratefully acknowledged. It involves a lot of effort but is largely invisible to all but the developers involved, and made the actual port to OS X and GNUStep much less painful.
</P>



<hr>
<address></address>
<!-- hhmts start --><!-- hhmts start -->Last modified: Sun Oct 18 20:31:41 PDT 2015 <!-- hhmts end --><!-- hhmts end -->
</body> </html>
